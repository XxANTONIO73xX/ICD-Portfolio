{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 5: Meta aprendizaje\n",
    "## Dataset: <a src=\"https://www.kaggle.com/datasets/sooyoungher/smoking-drinking-dataset/\">Smoking and Drinking Dataset with body signal</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la practica anterior generamos y analizamos 10 modelos de clasificación, utilizando validación cruzada y calculando su sensibilidad y espesificiadad. Esto con el objetivo de obtener el mejor modelo para clasificar si una persona es bebedora o no. Los 2 mejores modelos fueron los siguientes:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th colspan=4>Sin PCA</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Modelo</th>\n",
    "            <th>Exactitud</th>\n",
    "            <th>Sencibilidad</th>\n",
    "            <th>Especificidad</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Neural Net</td>\n",
    "            <td>72.300000</td>\n",
    "            <td>72.933333</td>\n",
    "            <td>71.433333</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Gaussian Process</td>\n",
    "            <td>72.150000</td>\n",
    "            <td>71.466667</td>\n",
    "            <td>72.833333</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score\n",
    "from numpy import mean\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-procesado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 991320 entries, 0 to 991345\n",
      "Data columns (total 24 columns):\n",
      " #   Column            Non-Null Count   Dtype   \n",
      "---  ------            --------------   -----   \n",
      " 0   sex               991320 non-null  category\n",
      " 1   age               991320 non-null  int64   \n",
      " 2   height            991320 non-null  int64   \n",
      " 3   weight            991320 non-null  int64   \n",
      " 4   waistline         991320 non-null  float64 \n",
      " 5   sight_left        991320 non-null  float64 \n",
      " 6   sight_right       991320 non-null  float64 \n",
      " 7   hear_left         991320 non-null  category\n",
      " 8   hear_right        991320 non-null  category\n",
      " 9   SBP               991320 non-null  float64 \n",
      " 10  DBP               991320 non-null  float64 \n",
      " 11  BLDS              991320 non-null  float64 \n",
      " 12  tot_chole         991320 non-null  float64 \n",
      " 13  HDL_chole         991320 non-null  float64 \n",
      " 14  LDL_chole         991320 non-null  float64 \n",
      " 15  triglyceride      991320 non-null  float64 \n",
      " 16  hemoglobin        991320 non-null  float64 \n",
      " 17  urine_protein     991320 non-null  category\n",
      " 18  serum_creatinine  991320 non-null  float64 \n",
      " 19  SGOT_AST          991320 non-null  float64 \n",
      " 20  SGOT_ALT          991320 non-null  float64 \n",
      " 21  gamma_GTP         991320 non-null  float64 \n",
      " 22  SMK_stat_type_cd  991320 non-null  category\n",
      " 23  DRK_YN            991320 non-null  category\n",
      "dtypes: category(6), float64(15), int64(3)\n",
      "memory usage: 149.4 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'smoking_driking_dataset_Ver01.csv')\n",
    "df = df.drop_duplicates()\n",
    "df.loc[:, [\"hear_left\", \"hear_right\", \"urine_protein\", \"SMK_stat_type_cd\"]] = df[\n",
    "    [\"hear_left\", \"hear_right\", \"urine_protein\", \"SMK_stat_type_cd\"]].astype(\"category\")\n",
    "df[\"DRK_YN\"] = df[\"DRK_YN\"].astype(\"category\")\n",
    "df[\"sex\"] = df[\"sex\"].astype(\"category\")  \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingenieria de Caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "for col in [\"sex\",\"hear_right\",\"hear_left\", \"DRK_YN\"]:\n",
    "    df.loc[:, [col]] = le.fit_transform(df[[col]])\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal = OrdinalEncoder()\n",
    "for col in [\"urine_protein\",\"SMK_stat_type_cd\"]:\n",
    "    df.loc[:, [col]] = ordinal.fit_transform(df[[col]])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "for col in [\"age\",\"height\"]:\n",
    "    df.loc[:, [col]] = scaler.fit_transform(df[[col]])\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "columns_to_encode = [\"urine_protein\", \"SMK_stat_type_cd\"]\n",
    "df[columns_to_encode] = encoder.fit_transform(df[columns_to_encode])\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robust = RobustScaler()\n",
    "columnas = ['weight', 'waistline', 'sight_left', 'sight_right','SBP', 'DBP', 'BLDS', 'tot_chole', 'HDL_chole', 'LDL_chole', 'triglyceride', 'hemoglobin','serum_creatinine', 'SGOT_AST', 'SGOT_ALT', 'gamma_GTP']\n",
    "for col in columnas:\n",
    "    df.loc[:, [col]] = robust.fit_transform(df[[col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separando la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Y = df.loc[df[\"DRK_YN\"]==1]\n",
    "df_Y = df_Y[:3000]\n",
    "df_N = df.loc[df[\"DRK_YN\"]==0]\n",
    "df_N = df_N[:3000]\n",
    "dataset = pd.concat([df_Y, df_N],axis=0)\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "Y = dataset[\"DRK_YN\"]  # Variable objetivo\n",
    "X = dataset.drop(columns=[\"DRK_YN\"])  # Características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiendo los algoritmos de inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = GaussianNB()\n",
    "clf3 = KNeighborsClassifier(n_neighbors=3)\n",
    "clf4 = svm.SVC()\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres = [\n",
    "    \"Gaussian Process\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Random Forest\",\n",
    "    \"Boosting\",\n",
    "    \"Bagging\",\n",
    "    \"Stacking\", \n",
    "    \"Voting\"\n",
    "]\n",
    "\n",
    "modelos = [\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(n_estimators=10, random_state=0),\n",
    "    RandomForestClassifier(n_estimators=10, random_state=0),\n",
    "    GradientBoostingClassifier(n_estimators=10),\n",
    "    BaggingClassifier(n_estimators=10),\n",
    "    StackingCVClassifier(classifiers=[clf1, clf2, clf3, clf4], meta_classifier=lr, random_state=42),\n",
    "    VotingClassifier(estimators=[('dt', clf1), ('nb', clf2), ('kNN', clf3), ('svm', clf4)],voting='hard')\n",
    "]\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimento 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\notebook 5 meta aprendizaje\\notebook.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spoon/Documents/GitHub/ICD-Portfolio/notebook%205%20meta%20aprendizaje/notebook.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m X_train, X_test \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39miloc[train_index], X\u001b[39m.\u001b[39miloc[test_index]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/spoon/Documents/GitHub/ICD-Portfolio/notebook%205%20meta%20aprendizaje/notebook.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m y_train, y_test \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39miloc[train_index], Y\u001b[39m.\u001b[39miloc[test_index]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/spoon/Documents/GitHub/ICD-Portfolio/notebook%205%20meta%20aprendizaje/notebook.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m modelo\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spoon/Documents/GitHub/ICD-Portfolio/notebook%205%20meta%20aprendizaje/notebook.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m y_pred \u001b[39m=\u001b[39m modelo\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spoon/Documents/GitHub/ICD-Portfolio/notebook%205%20meta%20aprendizaje/notebook.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m scores\u001b[39m.\u001b[39mappend(accuracy_score(y_test, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpc.py:741\u001b[0m, in \u001b[0;36mGaussianProcessClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    739\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown multi-class mode \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_class)\n\u001b[1;32m--> 741\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_estimator_\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[0;32m    743\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    744\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_marginal_likelihood_value_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(\n\u001b[0;32m    745\u001b[0m         [\n\u001b[0;32m    746\u001b[0m             estimator\u001b[39m.\u001b[39mlog_marginal_likelihood()\n\u001b[0;32m    747\u001b[0m             \u001b[39mfor\u001b[39;00m estimator \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_estimator_\u001b[39m.\u001b[39mestimators_\n\u001b[0;32m    748\u001b[0m         ]\n\u001b[0;32m    749\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpc.py:229\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_marginal_likelihood(theta, clone_kernel\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    227\u001b[0m \u001b[39m# First optimize starting from theta specified in kernel\u001b[39;00m\n\u001b[0;32m    228\u001b[0m optima \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 229\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_constrained_optimization(\n\u001b[0;32m    230\u001b[0m         obj_func, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_\u001b[39m.\u001b[39;49mtheta, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_\u001b[39m.\u001b[39;49mbounds\n\u001b[0;32m    231\u001b[0m     )\n\u001b[0;32m    232\u001b[0m ]\n\u001b[0;32m    234\u001b[0m \u001b[39m# Additional runs are performed from log-uniform chosen initial\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39m# theta\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_restarts_optimizer \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpc.py:474\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace._constrained_optimization\u001b[1;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constrained_optimization\u001b[39m(\u001b[39mself\u001b[39m, obj_func, initial_theta, bounds):\n\u001b[0;32m    473\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfmin_l_bfgs_b\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 474\u001b[0m         opt_res \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49moptimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[0;32m    475\u001b[0m             obj_func, initial_theta, method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m, jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, bounds\u001b[39m=\u001b[39;49mbounds\n\u001b[0;32m    476\u001b[0m         )\n\u001b[0;32m    477\u001b[0m         _check_optimize_result(\u001b[39m\"\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m\"\u001b[39m, opt_res)\n\u001b[0;32m    478\u001b[0m         theta_opt, func_min \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mx, opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    707\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    708\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    709\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 710\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    711\u001b[0m                            callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[0;32m    712\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    713\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    714\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:365\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    359\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    366\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    367\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:77\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[0;32m     76\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:71\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 71\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[0;32m     73\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpc.py:220\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace.fit.<locals>.obj_func\u001b[1;34m(theta, eval_gradient)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobj_func\u001b[39m(theta, eval_gradient\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    219\u001b[0m     \u001b[39mif\u001b[39;00m eval_gradient:\n\u001b[1;32m--> 220\u001b[0m         lml, grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_marginal_likelihood(\n\u001b[0;32m    221\u001b[0m             theta, eval_gradient\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, clone_kernel\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    222\u001b[0m         )\n\u001b[0;32m    223\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mlml, \u001b[39m-\u001b[39mgrad\n\u001b[0;32m    224\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpc.py:385\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace.log_marginal_likelihood\u001b[1;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[0;32m    381\u001b[0m     K \u001b[39m=\u001b[39m kernel(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX_train_)\n\u001b[0;32m    383\u001b[0m \u001b[39m# Compute log-marginal-likelihood Z and also store some temporaries\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39m# which can be reused for computing Z's gradient\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m Z, (pi, W_sr, L, b, a) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_posterior_mode(K, return_temporaries\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m eval_gradient:\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m Z\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpc.py:444\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace._posterior_mode\u001b[1;34m(self, K, return_temporaries)\u001b[0m\n\u001b[0;32m    442\u001b[0m W_sr_K \u001b[39m=\u001b[39m W_sr[:, np\u001b[39m.\u001b[39mnewaxis] \u001b[39m*\u001b[39m K\n\u001b[0;32m    443\u001b[0m B \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39meye(W\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m W_sr_K \u001b[39m*\u001b[39m W_sr\n\u001b[1;32m--> 444\u001b[0m L \u001b[39m=\u001b[39m cholesky(B, lower\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    445\u001b[0m \u001b[39m# Line 6\u001b[39;00m\n\u001b[0;32m    446\u001b[0m b \u001b[39m=\u001b[39m W \u001b[39m*\u001b[39m f \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_train_ \u001b[39m-\u001b[39m pi)\n",
      "File \u001b[1;32mc:\\Users\\spoon\\Documents\\GitHub\\ICD-Portfolio\\env\\Lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py:45\u001b[0m, in \u001b[0;36mcholesky\u001b[1;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mLAPACK reported an illegal value in \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m-th argument\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     41\u001b[0m                          \u001b[39m'\u001b[39m\u001b[39mon entry to \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPOTRF\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m-\u001b[39minfo))\n\u001b[0;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m c, lower\n\u001b[1;32m---> 45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcholesky\u001b[39m(a, lower\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, overwrite_a\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, check_finite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     46\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39m    Compute the Cholesky decomposition of a matrix.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m \n\u001b[0;32m     88\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m     c, lower \u001b[39m=\u001b[39m _cholesky(a, lower\u001b[39m=\u001b[39mlower, overwrite_a\u001b[39m=\u001b[39moverwrite_a, clean\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     90\u001b[0m                          check_finite\u001b[39m=\u001b[39mcheck_finite)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metricas_df = pd.DataFrame(columns=['Modelo', 'Exactitud', 'Sensibilidad', 'Especificidad'])\n",
    "for nombre, modelo in zip(nombres, modelos):\n",
    "    scores = []\n",
    "    sensibilidades = []\n",
    "    especificidades = []\n",
    "    matrices_de_confusion = []\n",
    "    for train_index, test_index in sss.split(X, Y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "        modelo.fit(X_train, y_train)\n",
    "        y_pred = modelo.predict(X_test)\n",
    "        scores.append(accuracy_score(y_test, y_pred))\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        matrices_de_confusion.append(conf_matrix)\n",
    "        # Calcular sensibilidad y especificidad\n",
    "        sensibilidad = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 0])\n",
    "        especificidad = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "        sensibilidades.append(sensibilidad)\n",
    "        especificidades.append(especificidad)\n",
    "    \n",
    "    matriz_confusion_promedio = sum(matrices_de_confusion) // len(matrices_de_confusion)\n",
    "    sensibilidad_promedio = mean(sensibilidades)\n",
    "    especificidad_promedio = mean(especificidades)\n",
    "    exactitud_promedio = mean(scores)\n",
    "    \n",
    "    # Agregar métricas al DataFrame\n",
    "    metrics = {\n",
    "        'Modelo': nombre,\n",
    "        'Exactitud': exactitud_promedio * 100,\n",
    "        'Sensibilidad': sensibilidad_promedio * 100,\n",
    "        'Especificidad': especificidad_promedio * 100\n",
    "    }\n",
    "    \n",
    "    metricas_df = pd.concat([metricas_df, pd.DataFrame(metrics, index=[0])], ignore_index=True)\n",
    "    \n",
    "    average_confusion_np = np.array(matriz_confusion_promedio)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(average_confusion_np, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Matriz de Confusión Promedio de '+nombre)\n",
    "    plt.show()\n",
    "\n",
    "metricas_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
